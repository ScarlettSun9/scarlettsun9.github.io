<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Paper name: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</p> <p><a href="https://arxiv.org/abs/2404.09403" rel="external nofollow noopener" target="_blank">Link of the paper</a></p> <p>This paper shows a method to combine pretrained language model with memory on specific knowledge. To tackle the hallucination problems of the language model and lack of knowledge update, the authors bring hybrid parametric (pretrained model) and non-parametric memory (retriever document index) to today‚Äôs seq2seq models. This kind of RAG model is suitable for the knowledge-intensive tasks and has achieved state-of-the-art results on many extractive QA datasets and knowledge-generation tasks. The model is also flexible to change the domain knowledge to update itself. It is a good way to ‚Äúinject‚Äù domain knowledge to the general model and enhance the quality and safety of the generated content.</p> <h2 id="approach--model">Approach &amp; Model</h2> <h3 id="niche">Niche</h3> <ul> <li>Pre-trained neural language models <ul> <li>cannot easily expand or revise their memory;</li> <li>can‚Äôt straightforwardly provide insight into the predictions;</li> <li>may produce ‚Äúhallucinations‚Äù</li> </ul> </li> <li>Hybrid models (parametric memory + non-parametric memories) can address some of these issues, yet have only explored open-domain extractive QA.</li> </ul> <h3 id="model">Model</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240419-1-480.webp 480w,/assets/img/20240419-1-800.webp 800w,/assets/img/20240419-1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240419-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> RAG approach overview </div> </div> <p>Actually it took me a while to clarify the two core models: RAG-Sequece Model &amp; RAG-Token Model. Now let‚Äôs focus on the formula and explain it step by step.</p> <h4 id="rag-sequence-model">RAG-Sequence Model</h4> \[p_{RAG-Sequence}(y|x) \approx \sum_{z\in top-k(p(¬∑|x))}p_{eta}(z|x)p_{\theta}(y|x,z)=\sum_{z\in top-k(p(¬∑|x))}p_{\eta}(z|x)\prod^N_i p_{\theta}(y_i|x,z.y_{1:i-1})\] <table> <tbody> <tr> <td>First, based on the input question \(x\), the retriever will find top-k documents that are most related to the input. This step is for $$p_{eta}(z</td> <td>x)$$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Next, for each selected document \(z\), the generator will generate token by token, with the condition of input question \(x\), selected document \(z\), and previously generated tokens \(y_{1:i-1}\). After genrating the whole result, multiplicate all the probabilities here $$p_{\theta}(y_i</td> <td>x,z.y_{1:i-1})\(to get the probability of generating the sentence provided the document\)z$$.</td> </tr> </tbody> </table> <p>Finally, sum the sentence probability for each document up and get the total probability of generating the answer \(y\) under the condition \(x\) with the help of the top-k documents.</p> <blockquote> <p>The RAG-Sequence model uses the same retrieved document to generate the complete <em>sequence</em>.</p> </blockquote> <h4 id="rag-token-model">RAG-Token Model</h4> \[p_{RAG-Token}(y|x)\approx \prod^N_i \sum_{z\in top-k(p(¬∑|x))}p_{\eta}(z|x)p_{\theta}(y_i|x,z,y_{1:i-1})\] <table> <tbody> <tr> <td>First, for each top-k document, compute its probability $$p_{\eta}(z</td> <td>x)\(with the probability to generate the next token based on question\)x\(, document\)z\(and previous tokens\)y_{1:i-1}\(. The result is the probability of the document\)z\(to generate the token\)y$$.</td> </tr> </tbody> </table> <p>Next is the summation. In this token generation step, summarize all the probabilities of the top-k documents.</p> <p>Finally, after generating the whole sentence, multiply the probability to generate each single word (the summation value of each step) to get the probability to generate the sentence.</p> <blockquote> <p>In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer.</p> </blockquote> <h3 id="retriever-dpr">Retriever: DPR</h3> \[p_{\eta}(z|x) \propto (d(z)^T q(x)) \quad d(z)=BERT_{d(z)}, \, q(x)=BERT_{q(x)}\] <table> <tbody> <tr> <td>The retriever uses document encoder \(q(x)\) and query encoder \(d(z)\) to get the list of \(k\) documents \(z\) with the highest prior probability $$p_{\eta}(z</td> <td>x)$$, which is a Maximum Inner Product Search (MIPS) problem.</td> </tr> </tbody> </table> <p>The retriever is pretrained as the <em>non-parametric memory</em> of the RAG model.</p> <h3 id="generator-bart">Generator: BART</h3> <p>BART-large, a pre-trained seq2seq transformer, is the generator of RAG. Its parameters is the <em>parametric memory</em> part.</p> <h2 id="experiment">Experiment</h2> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-7-480.webp 480w,/assets/img/20240417-7-800.webp 800w,/assets/img/20240417-7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Information flow illustration on sarcasm detection task (as well as sentiment analysis task). </div> </div> <ul> <li>4 tasks: <ul> <li>Open-domain QA</li> <li>Abstractive QA</li> <li>Jeopardy Qeusiton Generation</li> <li>Fact Verification</li> </ul> </li> <li>Key results: <ul> <li>RAG can combines the generation flexibility of the ‚Äúclosed-book‚Äù (parametric only) approaches and the performance of ‚Äúopen-book‚Äù retrieval-based approaches.</li> <li>(Jeopardy Questoin Generation shows how) parametric and non-parametric memories work together - the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory</li> <li>Generation Diversity: calculate the ratio of distinct ngrams to total ngrams generated by different models and find RAG-Seq‚Äôs generations are more diverse than RAG-Token and BART.</li> <li>People prefer RAG‚Äôs generation over purely parametric BART, finding RAG more factual and specific.</li> </ul> </li> </ul> <h2 id="impact">Impact</h2> <ol> <li> <strong>Parametric &amp; Non-parametric memories</strong> Opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them.</li> <li> <strong>Factuality</strong>: RAG models are more strongly grounded in real factual knowledge, making it ‚Äúhallucinate‚Äù less with generations that are more factual, and offers more control and interpretability.</li> <li> <strong>Downsides</strong>: RAG can be employed as a language model, and any potential external knowledge source for language model, will probably never be entirely factual and completely devoid of bias.</li> </ol> <blockquote> <p>AI systems could be employed to fight against misleading content and automated spam/phishing.</p> </blockquote> <h2 id="personal-comment">Personal Comment</h2> <p>RAG, a really smart method to combine real-world knowledge with pretrained language model, can avoid full-finetuning the model and is flexible enough to change domain knowledge. RAG-Sequence and RAG-Token were confusing at first. Even though I have made a step-by-step clarification, it would be better to check the original codes to see how the two models are implemented.</p> <p>(Okay I find most of the model innovations are smart üòÇ.)</p> </body></html>