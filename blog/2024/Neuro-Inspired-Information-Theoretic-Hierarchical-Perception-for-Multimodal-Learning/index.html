<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review üìù Neuro-Inspired ITHP for Multimodal Learning | Miaomiao Song </title> <meta name="author" content="Miaomiao Song"> <meta name="description" content="Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning - a new method for multimodal representation fusion"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://scarlettsun9.github.io/blog/2024/Neuro-Inspired-Information-Theoretic-Hierarchical-Perception-for-Multimodal-Learning/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Paper Review üìù Neuro-Inspired ITHP for Multimodal Learning",
            "description": "Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning - a new method for multimodal representation fusion",
            "published": "April 17, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Miaomiao</span> Song </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Paper Review üìù Neuro-Inspired ITHP for Multimodal Learning</h1> <p>Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning - a new method for multimodal representation fusion</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#approach-model">Approach &amp; Model</a> </div> <div> <a href="#niche">Niche</a> </div> <div> <a href="#concepts">Concepts</a> </div> <div> <a href="#cognitive-answer-for-multimodal-integration">Cognitive answer for multimodal integration</a> </div> <div> <a href="#information-bottleneck-ib">Information Bottleneck (IB)</a> </div> <div> <a href="#model-architecture">Model Architecture</a> </div> <div> <a href="#train-the-model">Train the Model</a> </div> <div> <a href="#experiment">Experiment</a> </div> <div> <a href="#sarcasm-detection">Sarcasm Detection</a> </div> <div> <a href="#varying-lagrange-multiplier">Varying Lagrange multiplier</a> </div> <div> <a href="#sentiment-analysis">Sentiment Analysis</a> </div> <div> <a href="#personal-comment">Personal Comment</a> </div> </nav> </d-contents> <p><a href="https://arxiv.org/abs/2404.09403" rel="external nofollow noopener" target="_blank">Link of the paper</a></p> <p>This paper demonstrates a method for fusing multimodal information with hierarchical information path way and information bottleneck structure. Experimental results show that the new method can outperform state-of-the-art benchmarks on sarcasm detection and sentiment analysis.</p> <h2 id="approach--model">Approach &amp; Model</h2> <h3 id="niche">Niche</h3> <p>Multimodal Fusion can be generally classified into two types: early fusion and late fusion. Early fusion underscores cross-modal information interaction. Common methods inlcude concatenating representations directly and adding representations together (which prerequisite the alignment among different modality). Yet the amount and importance of information differ across various modalities. One modality may play a much more important role in some context. Also for some people, they are more ‚Äúvisual‚Äù or ‚Äúaudio‚Äù when they get information than simply reading from text. These are cognitive features when humans processing information. So how to incorporate these to AI‚Äôs learning? The authors provide their tentative answer: ITHP.</p> <h3 id="concepts">Concepts</h3> <h4 id="cognitive-answer-for-multimodal-integration">Cognitive answer for multimodal integration</h4> <ul> <li>Information from different modalities forms connections in a specific order within the brain.</li> <li>Synaptic connections between different modality-receptive areas are reciprocal. This reciprocity enables information from various modalities to reciprocally exert feedback.</li> <li>Such a sequential and hierarchical processing approach allows the brain to start by processing information in a certain modality, gradually linking and processing information in other modalities, and finally analyzing and integrating information effectively in a coordinated manner.</li> <li>The brain selectively attends to relevant cues from each modality, extracts meaningful features, and combines them to form a more comprehensive and coherent representation of the multimodal stimulus.</li> </ul> <h4 id="information-bottleneck-ib">Information Bottleneck (IB)</h4> <ul> <li>This is mainly a tradeoff between compression and prediction in an information flow.</li> <li>Target: find a <strong>compact representation of one given state</strong> while <strong>preserving its predictive power with respect to another state</strong>.</li> </ul> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-1-480.webp 480w,/assets/img/20240417-1-800.webp 800w,/assets/img/20240417-1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> An illustration of IB implementation in multimodal representation learning. </div> </div> <p>This is a simple illustration for the Information Bottleneck structure for multimodal fusion. \(X_0\), \(X_1\) and \(X_2\) are the representations (matrices) of three modalities, from the most important (prime modality) to the least (remaining modalites). \(B_0\), \(B_1\) are two latent states, or so-called ‚Äúbottleneck‚Äù. According to the IB theroy, \(B_0\) should compact the information of \(X_0\) and at the same time maximize the relevant information of \(X_1\). In this way, \(B_0\) can be treated as the fusion of the first two modalities. Based on \(B_0\), \(B_1\) should similarly compact the information of \(B_0\) (the first two modalities) and preserve the information of \(X_2\).</p> <p>In this way, \(B_2\) is the final version of modality fusion. It is then used to predict the result \(Y\).</p> <p>To achieve the tradeoff between compressing a state X into the latent state B and maintaining relevant information of another state Y, the loss function can be like this (\(I\) is the mutual information):</p> \[L = I(X;B)-\beta I(B;Y)\] <h3 id="model-architecture">Model Architecture</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-2-480.webp 480w,/assets/img/20240417-2-800.webp 800w,/assets/img/20240417-2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> An illustration of the IHTP model structure. </div> </div> <p>First, to further explain the bottleneck structure loss function, we take a look at the latent state \(B_0\). Its optimization should subject to the following 3 equations:</p> \[I(X_0; X_1) ‚àí I(B_0; X_1) \leq \epsilon_1\] <p>Hidden state \(B_0\) covers as much relevant information in \(X_1\) as possible, so \(I(X_0; X_1) ‚àí \epsilon_1\) is a lower bound of \(I(B_0; X_1)\);</p> \[I(B_0; B_1) \leq \epsilon_2\] <p>Minimize \(I(B_0; B_1)\) to further compress the information of \(B0\) into \(B1\);</p> \[I(X_0; X_2) ‚àí I(B_1; X_2) \leq \epsilon_3\] <p>\(B_1\) is constructed to retains as much relevant information in \(X_2\) as possible, with \(I (X_0 ; X_2) ‚àí \epsilon_3\) as a lower bound.</p> <p>The general loss function (a Lagrangian function) is:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-3-480.webp 480w,/assets/img/20240417-3-800.webp 800w,/assets/img/20240417-3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>This can minimize the mutual information \(I(X_0;B_0)\) and \(I(B_0; B_1)\), and maximize the mutual information \(I(B_0; X_1)\) and \(I(B_1; X_2)\).</p> <p>Further, let‚Äôs see the detailed loss of the two bottlenecks: \(X_0\)-\(B_0\)-\(X_1\) and \(B_0\)-\(B_1\)-\(X_2\). The below equations are just an expansion of the above loss function. It considers deterministic distribution as \(p\) and the random probability distribution fitted by the neural network as \(q\).</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-4-480.webp 480w,/assets/img/20240417-4-800.webp 800w,/assets/img/20240417-4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <table> <tbody> <tr> <td>Here, *KL(p(X)</td> <td>¬†</td> <td>q(X))* is the Kullback-Leibler (KL) divergence between the two distributions <em>p(X)</em> and <em>q(X)</em>.</td> </tr> </tbody> </table> <h3 id="train-the-model">Train the Model</h3> <p>Loss functions again! The overall loss for the two-level hierarchy (three modality):</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-5-480.webp 480w,/assets/img/20240417-5-800.webp 800w,/assets/img/20240417-5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>If we need to train for the specific downstream task, the function is:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-6-480.webp 480w,/assets/img/20240417-6-800.webp 800w,/assets/img/20240417-6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="experiment">Experiment</h2> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-7-480.webp 480w,/assets/img/20240417-7-800.webp 800w,/assets/img/20240417-7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Information flow illustration on sarcasm detection task (as well as sentiment analysis task). </div> </div> <h3 id="sarcasm-detection">Sarcasm Detection</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-8-480.webp 480w,/assets/img/20240417-8-800.webp 800w,/assets/img/20240417-8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li>Dataset: the Multimodal Sarcasm Detection Dataset (MUStARD)</li> <li>Model to compare: fine-tuned multimodal sarcasm detection model (‚ÄúMSDM‚Äù) by Castro et al. (2019).</li> <li>Modality order: Visual - Text - Audio (Considering the binary classification results across the three modalities and taking into account the size of the embedding features for each modality (dv = 2048, dt = 768, da = 283))</li> <li>Results: <ul> <li>For single modality, the two methods‚Äô effects are almost the same.</li> <li>For two modalities, V-T is the best version. <ul> <li>MSDM V-A lower than V ‚û°Ô∏è struggled to effectively extract meaningful information by combining the embedding features from video and audio.</li> </ul> </li> <li>For three modalities, ITHP outperforms MSDM ‚û°Ô∏è succeeds to construct the effective information flow among the multimodality states for the sarcasm detection task.</li> </ul> </li> </ul> <h4 id="varying-lagrange-multiplier">Varying Lagrange multiplier</h4> <p>The authors also adjust the Lagrange multipliers, \(\gamma\) and \(\beta\), to see its influence on the precision and recall value.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-9-480.webp 480w,/assets/img/20240417-9-800.webp 800w,/assets/img/20240417-9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The upper right triangular part is brighter than the lower left triangular part ‚û°Ô∏è the model benefits more from a higher Œ≤ than a higher Œ≥ ‚û°Ô∏è retaining more relevant information from Text (X1) is more important for sarcasm detection.</p> <h3 id="sentiment-analysis">Sentiment Analysis</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-10-480.webp 480w,/assets/img/20240417-10-800.webp 800w,/assets/img/20240417-10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-11-480.webp 480w,/assets/img/20240417-11-800.webp 800w,/assets/img/20240417-11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/20240417-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li>Dataset: CMU-MOSI and CMU-MOSEI</li> <li>Models to compare: see the above tables</li> <li>Modality order: Text - Audio - Video (with em- bedding feature sizes of dt = 768, da = 74, and dv = 47 for text, audio, and video, re- spectively)</li> <li>Results: <ul> <li>ITHP model outperforms all the SOTA models in both BERT and DeBERTa incorporation settings.</li> <li>Significant strides in per-formance were observed with the integration of DeBERTa with the ITHP model, surpassing even human levels.</li> </ul> </li> </ul> <h2 id="personal-comment">Personal Comment</h2> <p>For me the ITHP model is important since I have used multimodal LLMs for emotion classification. In my own experiment I just followed the traditional way to concatenate two modalities‚Äô representations together, which works but not works well enough. I thought the fusion is a potential problem and I directly change it to late fusion. But this information bottleneck is a good structure to integrate. But again, how does it compare to the late fusion? Sometimes I even doubt the early fusion in theory. Is that procedures really useful or just our innocent manipulation of data and models? This always reminds me two theory: one is the allegory of the cave by Plato. All the models, when we don‚Äôt really know what happened inside, are shadows projected on to the wall by the real INTELLIGENCE; Another seems more optimistic: It doesn‚Äôt matter whether a cat is black or white, as long as it catches mice. Now most models have successfully caught the mice, including this ITHP.</p> <p>BTY, this is my first post!!! And it takes almost the same time to write a paper summary/review than read the paper! üòÇ</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ScarlettSun9/scarlettsun9.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2024 Miaomiao Song. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>