<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://scarlettsun9.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://scarlettsun9.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-24T03:09:15+00:00</updated><id>https://scarlettsun9.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://scarlettsun9.github.io/blog/2024/2024-04-19-RAG-Original-Paper/" rel="alternate" type="text/html" title=""/><published>2024-04-24T03:09:15+00:00</published><updated>2024-04-24T03:09:15+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/2024-04-19-RAG-Original-Paper</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/2024-04-19-RAG-Original-Paper/"><![CDATA[<p>Paper name: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</p> <p><a href="https://arxiv.org/abs/2404.09403">Link of the paper</a></p> <p>This paper shows a method to combine pretrained language model with memory on specific knowledge. To tackle the hallucination problems of the language model and lack of knowledge update, the authors bring hybrid parametric (pretrained model) and non-parametric memory (retriever document index) to today‚Äôs seq2seq models. This kind of RAG model is suitable for the knowledge-intensive tasks and has achieved state-of-the-art results on many extractive QA datasets and knowledge-generation tasks. The model is also flexible to change the domain knowledge to update itself. It is a good way to ‚Äúinject‚Äù domain knowledge to the general model and enhance the quality and safety of the generated content.</p> <h2 id="approach--model">Approach &amp; Model</h2> <h3 id="niche">Niche</h3> <ul> <li>Pre-trained neural language models <ul> <li>cannot easily expand or revise their memory;</li> <li>can‚Äôt straightforwardly provide insight into the predictions;</li> <li>may produce ‚Äúhallucinations‚Äù</li> </ul> </li> <li>Hybrid models (parametric memory + non-parametric memories) can address some of these issues, yet have only explored open-domain extractive QA.</li> </ul> <h3 id="model">Model</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240419-1-480.webp 480w,/assets/img/20240419-1-800.webp 800w,/assets/img/20240419-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240419-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> RAG approach overview </div> </div> <p>Actually it took me a while to clarify the two core models: RAG-Sequece Model &amp; RAG-Token Model. Now let‚Äôs focus on the formula and explain it step by step.</p> <h4 id="rag-sequence-model">RAG-Sequence Model</h4> \[p_{RAG-Sequence}(y|x) \approx \sum_{z\in top-k(p(¬∑|x))}p_{eta}(z|x)p_{\theta}(y|x,z)=\sum_{z\in top-k(p(¬∑|x))}p_{\eta}(z|x)\prod^N_i p_{\theta}(y_i|x,z.y_{1:i-1})\] <table> <tbody> <tr> <td>First, based on the input question \(x\), the retriever will find top-k documents that are most related to the input. This step is for $$p_{eta}(z</td> <td>x)$$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Next, for each selected document \(z\), the generator will generate token by token, with the condition of input question \(x\), selected document \(z\), and previously generated tokens \(y_{1:i-1}\). After genrating the whole result, multiplicate all the probabilities here $$p_{\theta}(y_i</td> <td>x,z.y_{1:i-1})\(to get the probability of generating the sentence provided the document\)z$$.</td> </tr> </tbody> </table> <p>Finally, sum the sentence probability for each document up and get the total probability of generating the answer \(y\) under the condition \(x\) with the help of the top-k documents.</p> <blockquote> <p>The RAG-Sequence model uses the same retrieved document to generate the complete <em>sequence</em>.</p> </blockquote> <h4 id="rag-token-model">RAG-Token Model</h4> \[p_{RAG-Token}(y|x)\approx \prod^N_i \sum_{z\in top-k(p(¬∑|x))}p_{\eta}(z|x)p_{\theta}(y_i|x,z,y_{1:i-1})\] <table> <tbody> <tr> <td>First, for each top-k document, compute its probability $$p_{\eta}(z</td> <td>x)\(with the probability to generate the next token based on question\)x\(, document\)z\(and previous tokens\)y_{1:i-1}\(. The result is the probability of the document\)z\(to generate the token\)y$$.</td> </tr> </tbody> </table> <p>Next is the summation. In this token generation step, summarize all the probabilities of the top-k documents.</p> <p>Finally, after generating the whole sentence, multiply the probability to generate each single word (the summation value of each step) to get the probability to generate the sentence.</p> <blockquote> <p>In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer.</p> </blockquote> <h3 id="retriever-dpr">Retriever: DPR</h3> \[p_{\eta}(z|x) \propto (d(z)^T q(x)) \quad d(z)=BERT_{d(z)}, \, q(x)=BERT_{q(x)}\] <table> <tbody> <tr> <td>The retriever uses document encoder \(q(x)\) and query encoder \(d(z)\) to get the list of \(k\) documents \(z\) with the highest prior probability $$p_{\eta}(z</td> <td>x)$$, which is a Maximum Inner Product Search (MIPS) problem.</td> </tr> </tbody> </table> <p>The retriever is pretrained as the <em>non-parametric memory</em> of the RAG model.</p> <h3 id="generator-bart">Generator: BART</h3> <p>BART-large, a pre-trained seq2seq transformer, is the generator of RAG. Its parameters is the <em>parametric memory</em> part.</p> <h2 id="experiment">Experiment</h2> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-7-480.webp 480w,/assets/img/20240417-7-800.webp 800w,/assets/img/20240417-7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Information flow illustration on sarcasm detection task (as well as sentiment analysis task). </div> </div> <ul> <li>4 tasks: <ul> <li>Open-domain QA</li> <li>Abstractive QA</li> <li>Jeopardy Qeusiton Generation</li> <li>Fact Verification</li> </ul> </li> <li>Key results: <ul> <li>RAG can combines the generation flexibility of the ‚Äúclosed-book‚Äù (parametric only) approaches and the performance of ‚Äúopen-book‚Äù retrieval-based approaches.</li> <li>(Jeopardy Questoin Generation shows how) parametric and non-parametric memories work together - the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory</li> <li>Generation Diversity: calculate the ratio of distinct ngrams to total ngrams generated by different models and find RAG-Seq‚Äôs generations are more diverse than RAG-Token and BART.</li> <li>People prefer RAG‚Äôs generation over purely parametric BART, finding RAG more factual and specific.</li> </ul> </li> </ul> <h2 id="impact">Impact</h2> <ol> <li><strong>Parametric &amp; Non-parametric memories</strong> Opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them.</li> <li><strong>Factuality</strong>: RAG models are more strongly grounded in real factual knowledge, making it ‚Äúhallucinate‚Äù less with generations that are more factual, and offers more control and interpretability.</li> <li><strong>Downsides</strong>: RAG can be employed as a language model, and any potential external knowledge source for language model, will probably never be entirely factual and completely devoid of bias.</li> </ol> <blockquote> <p>AI systems could be employed to fight against misleading content and automated spam/phishing.</p> </blockquote> <h2 id="personal-comment">Personal Comment</h2> <p>RAG, a really smart method to combine real-world knowledge with pretrained language model, can avoid full-finetuning the model and is flexible enough to change domain knowledge. RAG-Sequence and RAG-Token were confusing at first. Even though I have made a step-by-step clarification, it would be better to check the original codes to see how the two models are implemented.</p> <p>(Okay I find most of the model innovations are smart üòÇ.)</p>]]></content><author><name></name></author></entry><entry><title type="html">Retrieval-Augmented Generation for Large Language Models - A Survey</title><link href="https://scarlettsun9.github.io/blog/2024/RAG-For-LLM-Survey/" rel="alternate" type="text/html" title="Retrieval-Augmented Generation for Large Language Models - A Survey"/><published>2024-04-20T15:59:00+00:00</published><updated>2024-04-20T15:59:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/RAG-For-LLM-Survey</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/RAG-For-LLM-Survey/"><![CDATA[<p>This post shows how to add a table of contents in the beginning of the post.</p> <h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2> <p>To add a table of contents to a post, simply add</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">beginning</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div> <p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post.</p> <h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h2 id="table-of-contents-options">Table of Contents Options</h2> <p>If you want to learn more about how to customize the table of contents, you can check the <a href="https://github.com/toshimaru/jekyll-toc">jekyll-toc</a> repository.</p> <h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="RAG"/><category term="LLM"/><summary type="html"><![CDATA[test for sidebar post]]></summary></entry><entry><title type="html">Paper Review üìù &amp;amp; Play ü§π‚Äç‚ôÄÔ∏è - Two Papers on the Application of LangChain</title><link href="https://scarlettsun9.github.io/blog/2024/Two-Papers-About-LangChain-Application/" rel="alternate" type="text/html" title="Paper Review üìù &amp;amp; Play ü§π‚Äç‚ôÄÔ∏è - Two Papers on the Application of LangChain"/><published>2024-04-18T00:00:00+00:00</published><updated>2024-04-18T00:00:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/Two-Papers-About-LangChain-Application</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/Two-Papers-About-LangChain-Application/"><![CDATA[<h2 id="first-paper">First paper</h2> <h3 id="automating-customer-service-using-langchain-building-custom-open-source-gpt-chatbot-for-organizations"><a href="https://arxiv.org/abs/2310.05421">Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations</a></h3> <p>This paper talks about the procedure to use LangChain and LLM to build a customer-service chatbot and take the website of an education institution as an example. First the author use BeautifulSoup to scrape most of the data from the website as the dataset for the experiment. Then they choose Huggingface Instruct Embeddings as the text embedding, because it can generate tailored embeddings without further finetuning. Then they choose Google‚Äôs Flan T5 XXL as the language model an deploy the whole process using Gradio API.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240418-1-480.webp 480w,/assets/img/20240418-1-800.webp 800w,/assets/img/20240418-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240418-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Model Architecture </div> </div> <p>The steps of the service are: The embeddings of the dataset are sent to the model ‚û°Ô∏è The user input the question ‚û°Ô∏è get question embeddings and sent to the model ‚û°Ô∏è the model generate the answer ‚û°Ô∏è continue‚Ä¶</p> <p><strong>Several points worth noting:</strong></p> <ol> <li>The function of the datset. QA pairs are the best data for customer service llm, but it is only a small portion of the website. For other content of the website, it might not enough for the model to learn the domain knowledge. In other words, the data on a single website might not be enough for finetuning in my opinion;</li> <li>The authors also mentioned they used ‚ÄúLangChain‚Äù for finetuning, but this step is not in the paper;</li> <li>The comparison of models in this experiment is not useful. It would be better to compare different LLMs such as the Flan T5 here and LLaMA etc, instead of compare the effect of model with different size in the same series.</li> <li>Huggingface Instruct Embeddings - should give it a try.</li> </ol> <h2 id="second-paper">Second Paper</h2> <h3 id="an-analysis-of-large-language-models-and-langchain-in-mathematics-education"><a href="https://www.researchgate.net/publication/374590545_An_Analysis_of_Large_Language_Models_and_LangChain_in_Mathematics_Education">An Analysis of Large Language Models and LangChain in Mathematics Education</a></h3> <p>This paper is more about qualitative analysis on the mathematical problem solving ability of ChatGPT and LLMMathChain. Two types of math problems (word problem and numerical computational problem) and five error types are proposed; seven problems and the generated answeres are analyzed. The result is, in this experiment, ChatGPT‚Äôs accuracy on math solving is higher than LLMMathChain, which has generated answers containing four out of five types of errors. The authors have pointed out the essential difference between math problem solving and LLM generation, the former is deterministic and deductive, while the latter is probablistic and inductive. To achieve better result for LLM on math, more data are needed for the model to learn the deductive reasoning process. Besides, LLM + LangChain + Chain of Thought (CoT) + decision tree + etc. can be helpful for future mathematic models.</p> <p><strong>Comments:</strong></p> <p>The content here is more about qualitative analysis instead of algorithm or experimental discussion. So there aren‚Äôt too much to say. One thing needs to point out is the error types and examples. The authors only provides seven examples in the paper and then there are five error types, which seem not convincing or generalizable. It would be better to test more math examples and summarize the common mistakes models made.</p>]]></content><author><name></name></author><category term="LangChain"/><summary type="html"><![CDATA[One is about automating customer service, the other is in mathematical problem solving using LLMMathChain.]]></summary></entry><entry><title type="html">Paper Review üìù Neuro-Inspired ITHP for Multimodal Learning</title><link href="https://scarlettsun9.github.io/blog/2024/Neuro-Inspired-Information-Theoretic-Hierarchical-Perception-for-Multimodal-Learning/" rel="alternate" type="text/html" title="Paper Review üìù Neuro-Inspired ITHP for Multimodal Learning"/><published>2024-04-17T00:00:00+00:00</published><updated>2024-04-17T00:00:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/Neuro-Inspired-Information-Theoretic-Hierarchical-Perception-for-Multimodal-Learning</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/Neuro-Inspired-Information-Theoretic-Hierarchical-Perception-for-Multimodal-Learning/"><![CDATA[<p><a href="https://arxiv.org/abs/2404.09403">Link of the paper</a></p> <p>This paper demonstrates a method for fusing multimodal information with hierarchical information path way and information bottleneck structure. Experimental results show that the new method can outperform state-of-the-art benchmarks on sarcasm detection and sentiment analysis.</p> <h2 id="approach--model">Approach &amp; Model</h2> <h3 id="niche">Niche</h3> <p>Multimodal Fusion can be generally classified into two types: early fusion and late fusion. Early fusion underscores cross-modal information interaction. Common methods inlcude concatenating representations directly and adding representations together (which prerequisite the alignment among different modality). Yet the amount and importance of information differ across various modalities. One modality may play a much more important role in some context. Also for some people, they are more ‚Äúvisual‚Äù or ‚Äúaudio‚Äù when they get information than simply reading from text. These are cognitive features when humans processing information. So how to incorporate these to AI‚Äôs learning? The authors provide their tentative answer: ITHP.</p> <h3 id="concepts">Concepts</h3> <h4 id="cognitive-answer-for-multimodal-integration">Cognitive answer for multimodal integration</h4> <ul> <li>Information from different modalities forms connections in a specific order within the brain.</li> <li>Synaptic connections between different modality-receptive areas are reciprocal. This reciprocity enables information from various modalities to reciprocally exert feedback.</li> <li>Such a sequential and hierarchical processing approach allows the brain to start by processing information in a certain modality, gradually linking and processing information in other modalities, and finally analyzing and integrating information effectively in a coordinated manner.</li> <li>The brain selectively attends to relevant cues from each modality, extracts meaningful features, and combines them to form a more comprehensive and coherent representation of the multimodal stimulus.</li> </ul> <h4 id="information-bottleneck-ib">Information Bottleneck (IB)</h4> <ul> <li>This is mainly a tradeoff between compression and prediction in an information flow.</li> <li>Target: find a <strong>compact representation of one given state</strong> while <strong>preserving its predictive power with respect to another state</strong>.</li> </ul> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-1-480.webp 480w,/assets/img/20240417-1-800.webp 800w,/assets/img/20240417-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> An illustration of IB implementation in multimodal representation learning. </div> </div> <p>This is a simple illustration for the Information Bottleneck structure for multimodal fusion. \(X_0\), \(X_1\) and \(X_2\) are the representations (matrices) of three modalities, from the most important (prime modality) to the least (remaining modalites). \(B_0\), \(B_1\) are two latent states, or so-called ‚Äúbottleneck‚Äù. According to the IB theroy, \(B_0\) should compact the information of \(X_0\) and at the same time maximize the relevant information of \(X_1\). In this way, \(B_0\) can be treated as the fusion of the first two modalities. Based on \(B_0\), \(B_1\) should similarly compact the information of \(B_0\) (the first two modalities) and preserve the information of \(X_2\).</p> <p>In this way, \(B_2\) is the final version of modality fusion. It is then used to predict the result \(Y\).</p> <p>To achieve the tradeoff between compressing a state X into the latent state B and maintaining relevant information of another state Y, the loss function can be like this (\(I\) is the mutual information):</p> \[L = I(X;B)-\beta I(B;Y)\] <h3 id="model-architecture">Model Architecture</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-2-480.webp 480w,/assets/img/20240417-2-800.webp 800w,/assets/img/20240417-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> An illustration of the IHTP model structure. </div> </div> <p>First, to further explain the bottleneck structure loss function, we take a look at the latent state \(B_0\). Its optimization should subject to the following 3 equations:</p> \[I(X_0; X_1) ‚àí I(B_0; X_1) \leq \epsilon_1\] <p>Hidden state \(B_0\) covers as much relevant information in \(X_1\) as possible, so \(I(X_0; X_1) ‚àí \epsilon_1\) is a lower bound of \(I(B_0; X_1)\);</p> \[I(B_0; B_1) \leq \epsilon_2\] <p>Minimize \(I(B_0; B_1)\) to further compress the information of \(B0\) into \(B1\);</p> \[I(X_0; X_2) ‚àí I(B_1; X_2) \leq \epsilon_3\] <p>\(B_1\) is constructed to retains as much relevant information in \(X_2\) as possible, with \(I (X_0 ; X_2) ‚àí \epsilon_3\) as a lower bound.</p> <p>The general loss function (a Lagrangian function) is:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-3-480.webp 480w,/assets/img/20240417-3-800.webp 800w,/assets/img/20240417-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This can minimize the mutual information \(I(X_0;B_0)\) and \(I(B_0; B_1)\), and maximize the mutual information \(I(B_0; X_1)\) and \(I(B_1; X_2)\).</p> <p>Further, let‚Äôs see the detailed loss of the two bottlenecks: \(X_0\)-\(B_0\)-\(X_1\) and \(B_0\)-\(B_1\)-\(X_2\). The below equations are just an expansion of the above loss function. It considers deterministic distribution as \(p\) and the random probability distribution fitted by the neural network as \(q\).</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-4-480.webp 480w,/assets/img/20240417-4-800.webp 800w,/assets/img/20240417-4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <table> <tbody> <tr> <td>Here, *KL(p(X)</td> <td>¬†</td> <td>q(X))* is the Kullback-Leibler (KL) divergence between the two distributions <em>p(X)</em> and <em>q(X)</em>.</td> </tr> </tbody> </table> <h3 id="train-the-model">Train the Model</h3> <p>Loss functions again! The overall loss for the two-level hierarchy (three modality):</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-5-480.webp 480w,/assets/img/20240417-5-800.webp 800w,/assets/img/20240417-5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>If we need to train for the specific downstream task, the function is:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-6-480.webp 480w,/assets/img/20240417-6-800.webp 800w,/assets/img/20240417-6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="experiment">Experiment</h2> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-7-480.webp 480w,/assets/img/20240417-7-800.webp 800w,/assets/img/20240417-7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Information flow illustration on sarcasm detection task (as well as sentiment analysis task). </div> </div> <h3 id="sarcasm-detection">Sarcasm Detection</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-8-480.webp 480w,/assets/img/20240417-8-800.webp 800w,/assets/img/20240417-8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li>Dataset: the Multimodal Sarcasm Detection Dataset (MUStARD)</li> <li>Model to compare: fine-tuned multimodal sarcasm detection model (‚ÄúMSDM‚Äù) by Castro et al. (2019).</li> <li>Modality order: Visual - Text - Audio (Considering the binary classification results across the three modalities and taking into account the size of the embedding features for each modality (dv = 2048, dt = 768, da = 283))</li> <li>Results: <ul> <li>For single modality, the two methods‚Äô effects are almost the same.</li> <li>For two modalities, V-T is the best version. <ul> <li>MSDM V-A lower than V ‚û°Ô∏è struggled to effectively extract meaningful information by combining the embedding features from video and audio.</li> </ul> </li> <li>For three modalities, ITHP outperforms MSDM ‚û°Ô∏è succeeds to construct the effective information flow among the multimodality states for the sarcasm detection task.</li> </ul> </li> </ul> <h4 id="varying-lagrange-multiplier">Varying Lagrange multiplier</h4> <p>The authors also adjust the Lagrange multipliers, \(\gamma\) and \(\beta\), to see its influence on the precision and recall value.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-9-480.webp 480w,/assets/img/20240417-9-800.webp 800w,/assets/img/20240417-9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The upper right triangular part is brighter than the lower left triangular part ‚û°Ô∏è the model benefits more from a higher Œ≤ than a higher Œ≥ ‚û°Ô∏è retaining more relevant information from Text (X1) is more important for sarcasm detection.</p> <h3 id="sentiment-analysis">Sentiment Analysis</h3> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-10-480.webp 480w,/assets/img/20240417-10-800.webp 800w,/assets/img/20240417-10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240417-11-480.webp 480w,/assets/img/20240417-11-800.webp 800w,/assets/img/20240417-11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240417-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li>Dataset: CMU-MOSI and CMU-MOSEI</li> <li>Models to compare: see the above tables</li> <li>Modality order: Text - Audio - Video (with em- bedding feature sizes of dt = 768, da = 74, and dv = 47 for text, audio, and video, re- spectively)</li> <li>Results: <ul> <li>ITHP model outperforms all the SOTA models in both BERT and DeBERTa incorporation settings.</li> <li>Significant strides in per-formance were observed with the integration of DeBERTa with the ITHP model, surpassing even human levels.</li> </ul> </li> </ul> <h2 id="personal-comment">Personal Comment</h2> <p>For me the ITHP model is important since I have used multimodal LLMs for emotion classification. In my own experiment I just followed the traditional way to concatenate two modalities‚Äô representations together, which works but not works well enough. I thought the fusion is a potential problem and I directly change it to late fusion. But this information bottleneck is a good structure to integrate. But again, how does it compare to the late fusion? Sometimes I even doubt the early fusion in theory. Is that procedures really useful or just our innocent manipulation of data and models? This always reminds me two theory: one is the allegory of the cave by Plato. All the models, when we don‚Äôt really know what happened inside, are shadows projected on to the wall by the real INTELLIGENCE; Another seems more optimistic: It doesn‚Äôt matter whether a cat is black or white, as long as it catches mice. Now most models have successfully caught the mice, including this ITHP.</p> <p>BTY, this is my first post!!! And it takes almost the same time to write a paper summary/review than read the paper! üòÇ</p>]]></content><author><name></name></author><category term="AI"/><category term="multimodal"/><summary type="html"><![CDATA[Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning - a new method for multimodal representation fusion]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://scarlettsun9.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry><entry><title type="html">a post with advanced image components</title><link href="https://scarlettsun9.github.io/blog/2024/advanced-images/" rel="alternate" type="text/html" title="a post with advanced image components"/><published>2024-01-27T11:46:00+00:00</published><updated>2024-01-27T11:46:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/advanced-images/"><![CDATA[<p>This is an example post with advanced image components.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what advanced image components could look like]]></summary></entry><entry><title type="html">a post with vega lite</title><link href="https://scarlettsun9.github.io/blog/2024/vega-lite/" rel="alternate" type="text/html" title="a post with vega lite"/><published>2024-01-27T00:20:00+00:00</published><updated>2024-01-27T00:20:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/vega-lite/"><![CDATA[<p>This is an example post with some <a href="https://vega.github.io/vega-lite/">vega lite</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">vega_lite
</span><span class="sb">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}
</code></pre> <p>This plot supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included vega lite code could look like]]></summary></entry><entry><title type="html">a post with geojson</title><link href="https://scarlettsun9.github.io/blog/2024/geojson-map/" rel="alternate" type="text/html" title="a post with geojson"/><published>2024-01-26T17:57:00+00:00</published><updated>2024-01-26T17:57:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/geojson-map</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/geojson-map/"><![CDATA[<p>This is an example post with some <a href="https://geojson.org/">geojson</a> code. The support is provided thanks to <a href="https://leafletjs.com/">Leaflet</a>. To create your own visualization, go to <a href="https://geojson.io/">geojson.io</a>.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">geojson
</span><span class="sb">{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {},
      "geometry": {
        "coordinates": [
          [
            [
              -60.11363029935569,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -2.904625022183211
            ]
          ]
        ],
        "type": "Polygon"
      }
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-geojson">{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {},
      "geometry": {
        "coordinates": [
          [
            [
              -60.11363029935569,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -2.904625022183211
            ]
          ]
        ],
        "type": "Polygon"
      }
    }
  ]
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><category term="maps"/><summary type="html"><![CDATA[this is what included geojson code could look like]]></summary></entry><entry><title type="html">a post with echarts</title><link href="https://scarlettsun9.github.io/blog/2024/echarts/" rel="alternate" type="text/html" title="a post with echarts"/><published>2024-01-26T16:03:00+00:00</published><updated>2024-01-26T16:03:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/echarts</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/echarts/"><![CDATA[<p>This is an example post with some <a href="https://echarts.apache.org/">echarts</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">echarts
</span><span class="sb">{
  "title": {
    "text": "ECharts Getting Started Example"
  },
  "responsive": true,
  "tooltip": {},
  "legend": {
    "top": "30px",
    "data": ["sales"]
  },
  "xAxis": {
    "data": ["Shirts", "Cardigans", "Chiffons", "Pants", "Heels", "Socks"]
  },
  "yAxis": {},
  "series": [
    {
      "name": "sales",
      "type": "bar",
      "data": [5, 20, 36, 10, 10, 20]
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-echarts">{
  "title": {
    "text": "ECharts Getting Started Example"
  },
  "responsive": true,
  "tooltip": {},
  "legend": {
    "top": "30px",
    "data": ["sales"]
  },
  "xAxis": {
    "data": ["Shirts", "Cardigans", "Chiffons", "Pants", "Heels", "Socks"]
  },
  "yAxis": {},
  "series": [
    {
      "name": "sales",
      "type": "bar",
      "data": [5, 20, 36, 10, 10, 20]
    }
  ]
}
</code></pre> <p>Note that this library offer support for both light and dark themes. You can switch between them using the theme switcher in the top right corner of the page.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included echarts code could look like]]></summary></entry><entry><title type="html">a post with chart.js</title><link href="https://scarlettsun9.github.io/blog/2024/chartjs/" rel="alternate" type="text/html" title="a post with chart.js"/><published>2024-01-26T01:04:00+00:00</published><updated>2024-01-26T01:04:00+00:00</updated><id>https://scarlettsun9.github.io/blog/2024/chartjs</id><content type="html" xml:base="https://scarlettsun9.github.io/blog/2024/chartjs/"><![CDATA[<p>This is an example post with some <a href="https://www.chartjs.org/">chart.js</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">chartjs
</span><span class="sb">{
  "type": "line",
  "data": {
    "labels": [
      "January",
      "February",
      "March",
      "April",
      "May",
      "June",
      "July"
    ],
    "datasets": [
      {
        "label": "# of bugs",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 1,
        "pointHoverRadius": 5,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [
          65,
          59,
          80,
          81,
          56,
          55,
          40
        ],
        "spanGaps": false
      }
    ]
  },
  "options": {}
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [
      "January",
      "February",
      "March",
      "April",
      "May",
      "June",
      "July"
    ],
    "datasets": [
      {
        "label": "# of bugs",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 1,
        "pointHoverRadius": 5,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [
          65,
          59,
          80,
          81,
          56,
          55,
          40
        ],
        "spanGaps": false
      }
    ]
  },
  "options": {}
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">chartjs
</span><span class="sb">{
  "type": "doughnut",
  "data": {
    "labels": [
      "Red",
      "Blue",
      "Yellow"
    ],
    "datasets": [
      {
        "data": [
          300,
          50,
          100
        ],
        "backgroundColor": [
          "#FF6384",
          "#36A2EB",
          "#FFCE56"
        ],
        "hoverBackgroundColor": [
          "#FF6384",
          "#36A2EB",
          "#FFCE56"
        ]
      }
    ]
  },
  "options": {}
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-chartjs">{
  "type": "doughnut",
  "data": {
    "labels": [
      "Red",
      "Blue",
      "Yellow"
    ],
    "datasets": [
      {
        "data": [
          300,
          50,
          100
        ],
        "backgroundColor": [
          "#FF6384",
          "#36A2EB",
          "#FFCE56"
        ],
        "hoverBackgroundColor": [
          "#FF6384",
          "#36A2EB",
          "#FFCE56"
        ]
      }
    ]
  },
  "options": {}
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included chart.js code could look like]]></summary></entry></feed>